\chapter{Week 5 Lectures Week of 25th Sept}

\section{Multivariate functions}

Multivariate functions map inputs in $n dimensions$ to output in $m$ dimensions
\[
  f: \mathbb{R}^n \to \mathbb{R}^m
\]  

Such functions include both implicit and parameterized functions
\begin{itemize}
   \item Parameterized equations such as a parameterized curve $r: \mathbb{R}^1 \to \mathbb{R}^3$ 
   \item Implicit equations such as a parameterized curve $x^2 + y^2 = 4$
\end{itemize}

However, MATH-1410 will work with the general case, where any multivariate function can be thought of as \[
  f; \mathbb{R}^n \to \mathbb{R}^m
\]  

\subsection{The partial derivative}

\begin{framed}
   \textbf{Definition:} The partial derivative wrt a variable

   \[
     f \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 
       f_1 (x,y) \\
       f_2 (x,y)
     \end{pmatrix}
   \] 

   The partial derivatives are \[
     \frac{\delta f_1}{\delta x}, \frac{\delta f_2}{\delta x},\frac{\delta f_1}{\delta y}, \frac{\delta f_2}{\delta y}
   \] 
\end{framed}

\subsection{The Derivative}

The derivative $[Df]$ can be thought of as a data structure, where the element \[
   [Df]_{j, i}
\] 

Corresponds to the partial derivative of the $j-th$ output and $ i-th$ input

For example, for \[
  f \begin{pmatrix} 
    x \\ y  
  \end{pmatrix} = \begin{pmatrix} 
    x^2 y - 2xy \\
    x^3 + 3y^2
  \end{pmatrix}
\] 

The derivative is \[
   [Df] = \begin{bmatrix} 
      2xy - 2y & x^2 - 2x \\
      3x^2 & 6y \\
   \end{bmatrix}
\] 

Given a point, we can evaluate the derivative at that point. For example, at point $x = 1, y =2$, we get
\[
   [Df]_{\substack{x = 1 \\ y = 2}} = \begin{bmatrix} 
      0 & -1 \\ 3 & 12
   \end{bmatrix}
\] 

Given the derivative evaluated at a point, we can directly read off the sensitivity of the $j_th$ output to the $i-th$ input. 


In this case, the $2nd$ output is $12$ times as sensitive as the $1st$ output to the $2nd$ input

\section{The big lift!!} 
\begin{framed}
   \textbf{IMPORTANT}: the derivative is a linear transformation that takes \emph{vectors of rates of change of inputs} to \emph{vectors of rates of change of outputs}

   \[
      \underline{l} = [Df]_{ \underline{a}} \underline{h}
   \] 
\end{framed}

\subsection{Intuitive understanding of matrix as a linear transformation}

Consider for a function \[
  f : \mathbb{R}^n \to \mathbb{R}^m
\]  

If the function as derivative $[Df]_{ \underline{a}}$, the derivative is of the form \[
   \left[
   \begin{array}{c | c | c | c}
      & & &  \\
      \frac{\delta}{\delta x_1} & \frac{\delta}{\delta x_2} & \hdots&\frac{\delta}{\delta x_n} \\
      & & &  \\
   \end{array} \right]
\] 

The $i-th$ column of $[Df]_{ \underline{a}}$ describes how $f( \underline{a})$ changes with a unit change to $ \underline{a} $ in the $i-th$ dimension \\

In other words, the $i-th$ column describes a parametric curve of $f( \underline{a})$ in $m-th$ dimensional space, i.e. \[
  \gamma: \mathbb{R}^1 \to \mathbb{R}^m
\] 

\subsection{The derivative linear transformation as a weighted sum of columns}
Recall that all matrix transformations are a weighted linear combination of the columns of the matrix \\

Applied to the derivative, when the derivative $ [Df]_{ \underline{a}}$ maps the rates of change of inputs \[
   \underline{l} = [Df]_{ \underline{a} } \underline{h}
\],

The rates of change of outputs $ \underline{l}$ is the column-wise combination of $ [Df]_{ \underline{a}}$, weighted by the elements of $ \underline{h}$

\subsection{An example with polar coordinates}

For the function $P$ that maps polar coordinates $ \begin{pmatrix} r \\ \theta \end{pmatrix} $ to Euclidean coordinates $ \begin{pmatrix} x \\ y \end{pmatrix} $, 
\[
  \begin{pmatrix} x \\y \end{pmatrix}  = P \begin{pmatrix} r \\ \theta \end{pmatrix}  = \begin{pmatrix}
    r cos \theta \\
    r sin \theta
  \end{pmatrix}
\] 

The derivative is given by \[
   [DP] = \begin{bmatrix} 
      cos \theta & - r sin \theta \\
      sin \theta &  r cos \theta \\
   \end{bmatrix}
\] 

Picking any point, such as $(1, \frac{ \pi}{6}$, \[
   [DP]_{1, \frac{\pi}{6}} = \begin{pmatrix} 
      \frac{\sqrt{3}}{2} & -\frac{1}{2}   \\
      \frac{1}{2} & \frac{\sqrt{3}}{2}   \\
   \end{pmatrix}
\] 

\subsection{Another example}
For a parabola \[
  z = x^2 + y^2
\] 

Note that $z$ is a function of $x and y$, in the form
 \[
  f: \mathbb{R}^2 \to \mathbb{R}
\]  

The derivative is
\[
   [Dz] = \begin{bmatrix} 
      2x & 2y
   \end{bmatrix}
\] 

At point $(1, 2)$, \[
   [Dz]_{ \underline{a} } = \begin{bmatrix} 
      2 & 4  
   \end{bmatrix}
\] 

Writing the system in a parametric form, in the form \[
  f: \mathbb{R}^3 \to \mathbb{R}^2
\] 
\[
  G \begin{pmatrix} s \\ t \end{pmatrix} = \begin{pmatrix} x \\ y \\ z \end{pmatrix}  = \begin{pmatrix} 
    s \\ t \\ s^2 + t^2  
  \end{pmatrix}
\] 

The derivative $[DG]$ is
 \[
    [DG] = \begin{bmatrix} 
       1 & 0 \\  
       0 & 1 \\  
       2s & 2t
    \end{bmatrix}
\] 

\subsection{Inferring information from a derivative}
Given a derivative $[Df]_{ \underline{a}}$, without knowing the underlying function $f$ 

\[
   [Df]_{ \underline{a}} = \begin{bmatrix} 
      2 & 2 & 1\\
      -3 & 4 & 0\\
      1 & 7 & 3\\
      0 & 8 & 4\\
   \end{bmatrix}
\] 

What can be inferred is
\begin{itemize}
   \item the function $f$ takes $3$ inputs and returns $4$ outputs
   \item the 2nd output is completely insensitive to the third input
   \item the 4th output is completely insensitive to the first input
   \item when all inputs are increasing at unit rate, all outputs are increasing\[
         \underline{h} = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix} , [Df]_{ \underline{a}} \underline{h} = \begin{pmatrix} 5 \\ 1 \\ 11 \\ 12 \end{pmatrix} 
   \] 
\end{itemize}

\subsection{Another example}
For a function
\[
  \underline{y} = f ( \underline{x}) = A \underline{x}
\] 

It can be shown that 
\[
   [Df] = A
\] 

More importantly, if 
\[
  \underline{y} = \begin{bmatrix} 
     1 & 5 \\ 2 & -4  
  \end{bmatrix} \underline{x}
\] 

Then the inverse is\[
  \underline{x} = \begin{bmatrix} 
     \frac{2}{7} & \frac{5}{14} \\
     \frac{1}{7} & - \frac{1}{14} \\
  \end{bmatrix} \underline{y}
\] 

\textbf{Important}:
\[
  \frac{\partial y_2}{\partial x_1} = 2, \quad  \frac{\partial x_1}{\partial y_2} = \frac{5}{14}
\] 


\subsection{A Taylor perspective}

Recall from single variable calculus that for a function $f$, at very small values of $h$,
\[
  f(x + h) = f(x) + f'(x)h + \hdots
\] 

In the multivariate case

\[
   f( \underline{x} + \underline{h}) = f( \underline{x}) + [Df] \underline{h} + \hdots
\] 

Where \begin{itemize}
   \item $f( \underline{x})$ is the $0-th$ term
   \item $[Df] \underline{h}$ is the linear term
\end{itemize}

This \textbf{is} the definition of the derivative!! $\rightarrow$ it is the coefficient of the linear term in the Taylor series

\subsubsection{Another example}

For a function
\[
  S(A) = A^2
\] 
Where $A$ is a $2 by 2$ matrix, the derivative $[DS]$ would have $4$ inputs and $4$ outputs

\[
  S(A+H) = (A+H)^2 = A^2 + AH + HA + H^2
\] 
\begin{align*}
   S(A+H) &= S(A) + [DS]_{A}H \quad + \hdots\\
          &= A^2  +  AH + HA \quad +  H^2
\end{align*}

Hence,
\[
   [DS]_{A}H = AH + HA
\] 
 

